# R KNN 算法学习
# K最近邻分类算法，最简单的算法之。
#一如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。

library( caret)
iris_dataset <- iris
iris_index <- createDataPartition( iris_dataset$Species, times = 1, p = 0.35, list = F, groups = min(5, length( iris_dataset$Species)))
iris_train <- iris_dataset[ iris_index,]
iris_test <- iris_dataset[ -iris_index,]

knn( train = iris_train[ ,1:4 ], test = iris_test[ ,1:4 ], cl = iris_train[ ,5],
     k = 4, prob = T)

# 验算
test_knn <- function( input = 1, k = 1){
  x <- (traindata[,1:4]-testdata[rep(n,75),1:4])^2
  traindata$dist1 <- apply(x,1,function(x) sqrt(sum(x)))
}


# 从UCI Database 中获取数据，Get car data from UCI Database
# dataset info:
# PRICE overall price 
#   buying buying price 
#   maint price of the maintenance 
# TECH technical characteristics 
#  COMFORT comfort 
#    doors number of doors 
#    persons capacity in terms of persons to carry 
#    lug_boot the size of luggage boot 
#    safety estimated safety of the car 
car <- read.table(  
  'http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data',  
  sep=',',  
  header = F,
  stringsAsFactors = F
)
names_car <- c( "buying" , "maint", "doors", "persons", "lug_boot", "safety", "Class_Values")
names( car) <- names_car
str( car) # 查看到7个变量均为 Factor格式，需要转换格式并 进行哑变量处理
prop.table(table(car$Class_Values)) # 查看各个 Class_Value 占比情况


# 哑变量处理
# ------ test ------
test_index <- createDataPartition( car$Class_Values, time = 1, p = 0.5, list = F )
test <- car[ test_index, ]
Class_Values <- as.character( test$Class_Values)
test_for_model <- data.frame( predict( dummyVars( ~., data = test[ , 1:6] ), newdata = test[ , 1:6]), test$Class_Values)

result <- knn( train = test_for_model[ 1:800,1:21], test =  test_for_model[ 801:865,1:21], cl =  test_for_model[ 1:800,22],
     k = 5, prob = TRUE)
prop.table( table( test_for_model[ 801:865,22] == result))
# ------ test ------

# 哑变量处理 
# 先将 Class_Values 赋值出来
Class_Values <- as.character( car$Class_Values)
# 将Class_Values 剔除后进行哑变量处理，为了保证分类项不受变化。
dmy <- dummyVars( ~., data = car[ , 1:6] ) 
# 将哑变量处理后的数据和分类项组合成DF
car_1 <- data.frame( predict( dmy, newdata = car[ , 1:6] ), Class_Values = car$Class_Values)

# 分层抽样，抽取80%的数据，抽取1次
car_index <- createDataPartition( car_1$Class_Values, time = 1, p = 0.8, list = F )
# 划分训练集和测试集
car_train <- car_1[ car_index, ]
car_test <- car_1[ -car_index, ]

# KNN 建模，设置3和k(邻居)
result <- knn( train = car_train[ ,1:21], test =  car_test[ ,1:21], cl =  car_train[ ,22],
               k = 3, prob = TRUE)
# 查看与测试集分类项的百分比
prop.table( table( car_test[ ,22] == result))

# 验算过程
Ceshi <- function( n = 1, k = 3){
  # 计算第n和样本测试集与训练集样本的距离(欧式距离)
  x <- ( car_train[ ,1:21] - car_test[ rep( n, dim(car_train)[1]), 1:21 ])^2
  car_train$dist1 <- apply( x, 1, function(x) sqrt( sum( x) ) )
  # 对距离进行升序排序，选择最近的K个邻居
  mydata <- car_train[ order( car_train$dist1)[1:k],22:23]
  # 统计不同类别的频数
  result <- data.frame( sort( table( mydata$Species),decreasing = T))
  # 给出最后的预测结果
  return( result[1,1])
}

# 最终发现，全文本的数据集，哑变量后全部变为 0，1，后续计算无意义
# 或者可以说，同一类型的数据，映射到2维坐标轴上基本上都是在同一个点上。
# 全文本数据如何做分类？

# 优化 查看相关系数，删除某些不必要的变量？
# 计算相关系数：涉及字符型 - 貌似有个计算相关系数的方法是可以涉及到字符串的。如果不行，则需要哑变量处理后，再计算相关系数。
cor( data.frame( predict( dmy, newdata = car[ , 1:6] ), Class_Values = as,numeric( car$Class_Values) ) )


# 尝试用iris 数据集做KNN分类。
iris_dataset <- iris

cor( data.frame( predict( dmy, newdata = car[ , 1:6] ), Class_Values = as,numeric( car$Class_Values) ) )

iris_index <- createDataPartition( iris_dataset$Species, times = 1, p = 0.35, list = F, groups = min(5, length( iris_dataset$Species)))
iris_train <- iris_dataset[ iris_index,]
iris_test <- iris_dataset[ -iris_index,]

result_iris <- knn( train = iris_train[ ,1:4 ], test = iris_test[ ,1:4 ], cl = iris_train[ ,5],
     k = 3, prob = T)

# 验算
Ceshi_iris <- function( n = 1, k = 3){
  # 计算第n和样本测试集与训练集样本的距离(欧式距离)
  x <- ( iris_train[ ,1:4] - car_test[ rep( n, dim( iris_train)[1]), 1:4 ])^2
  iris_train$dist1 <- apply( x, 1, function(x) sqrt( sum( x) ) )
  # 对距离进行升序排序，选择最近的K个邻居
  mydata <- iris_train[ order( iris_train$dist1)[1:k],5:6]
  # 统计不同类别的频数
  result <- data.frame( sort( table( mydata$Species),decreasing = T))
  # 给出最后的预测结果
  return( result[1,1])
}

prop.table( table( iris_test[ ,5] == result_iris))



# 利用朴素贝叶斯方法对 car数据进行分类
library( e1071)
car_NB <- read.table(  
  'http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data',  
  sep=',',  
  header = F,
  stringsAsFactors = F
)
names_car_NB <- c( "buying" , "maint", "doors", "persons", "lug_boot", "safety", "Class_Values")
names( car_NB) <- names_car_NB

car_NB_index <- createDataPartition( car_NB$Class_Values, time = 1, p = 0.5, list = F )
# 划分训练集和测试集
car_NB_train <- car_NB[ car_NB_index, ]
car_NB_test <- car_NB[ -car_NB_index, ]

naiveBayes.model <- naiveBayes( Class_Values~., data=car_NB_train)
carTR_predict <- predict( naiveBayes.model, newdata=car_NB_train) # 训练集数据
carTE_predict <- predict( naiveBayes.model, newdata=car_NB_test) # 测试集数据
